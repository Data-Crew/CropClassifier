{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook demonstrates how to retrieve Sentinel2 base imagery for pixel-based crop type predictions.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up imports\n",
    "project_root = os.path.abspath(\"..\") \n",
    "sys.path.append(project_root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.datasources import (\n",
    "    query_stac_api, \n",
    "    list_folders_second_to_deepest_level, \n",
    "    get_existing_data, \n",
    "    get_directory_size,\n",
    "    unique_indices, process_result\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data to be processed into chunks of bboxes & years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to be processed into chunks of bboxes & years\n",
    "\n",
    "# Specify the root path\n",
    "CDL_path = \"../data/CDL_unique_scene.parquet/\"\n",
    "#CDL_path = \"../data/CDL_samples.parquet/\"   \n",
    "\n",
    "# Define the target depth\n",
    "target_depth = 2  # Second-to-deepest level\n",
    "\n",
    "# Get the list of folders at the target depth recursively\n",
    "folder_paths = list_folders_second_to_deepest_level(CDL_path, [], 0, target_depth)\n",
    "\n",
    "# Convert folder paths to partition value dictionaries\n",
    "df_train_partition_values_list = []\n",
    "for path in folder_paths:\n",
    "    segments = path.split(os.sep)\n",
    "    partition_values = {}\n",
    "    for segment in segments:\n",
    "        if '=' in segment:\n",
    "            key, value = segment.split('=')\n",
    "            partition_values[key] = value\n",
    "    df_train_partition_values_list.append(partition_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_partition_values_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrive existing data (to avoid reprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is only needed/used for restarting processing after stopping for some reason (start where code left off)\n",
    "# e.g. full file\n",
    "s2_file_path = '../data/dbfs_s2_unique_scene.parquet/'\n",
    "\n",
    "print(get_existing_data(s2_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_size_gb = get_directory_size(s2_file_path) / (1024 ** 3)\n",
    "    print(f\"Size of '{s2_file_path}' is {dir_size_gb:.2f} GB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Directory does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engine/Loop to retrieve and sample Sentinel-2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "assets_list = ['scl', 'coastal', 'blue', 'green', 'red', 'rededge1', 'rededge2', 'rededge3', 'nir', 'nir08', 'nir09', 'swir16', 'swir22']\n",
    "scl_exclude_list = [0, 1, 7, 8, 9, 11] # ignore certain scl layer values....\n",
    "# SCL_color_mappings = {\n",
    "#   0: # No Data (Missing data) - black  \n",
    "#   1: # Saturated or defective pixel - red \n",
    "#   2: # Topographic casted shadows (\"Dark features/Shadows\" for data before 2022-01-25) - very dark grey\n",
    "#   3: # Cloud shadows - dark brown\n",
    "#   4: # Vegetation - green\n",
    "#   5: # Not-vegetated - dark yellow\n",
    "#   6: # Water (dark and bright) - blue\n",
    "#   7: # Unclassified - dark grey\n",
    "#   8: # Cloud medium probability - grey\n",
    "#   9: # Cloud high probability - white\n",
    "#   10: # Thin cirrus - very bright blue\n",
    "#   11: # Snow or ice - very bright pink\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "existing_s2_dates = get_existing_data(s2_file_path)\n",
    "s2_file_path = '../data/s2_unique_scene.parquet/' # output path\n",
    "\n",
    "lock = multiprocessing.Lock()\n",
    "\n",
    "start_time = time.time()\n",
    "successful_scenes = 0\n",
    "failed_scenes = 0\n",
    "\n",
    "for el in df_train_partition_values_list[:1]:  \n",
    "    bbox = el['bbox']\n",
    "    year = el['year']\n",
    "    CDL_parts_path = os.path.join(CDL_path, f\"bbox={bbox}\", f\"year={year}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"[INFO] Trying delimiter ', ': {bbox}\")\n",
    "        bbox_tuple = tuple([int(x) for x in bbox.split(', ')])\n",
    "        print(\"[OK] Parsed bbox using delimiter ', '\")\n",
    "    except ValueError:\n",
    "        print(f\"[INFO] Trying delimiter ',': {bbox}\")\n",
    "        bbox_tuple = tuple([int(x) for x in bbox.split(',')])\n",
    "        print(\"[OK] Parsed bbox using delimiter ','\")\n",
    "\n",
    "    results = query_stac_api(\n",
    "        bounds=bbox_tuple,\n",
    "        epsg4326=False,\n",
    "        start_date=f\"{year}-01-01T00:00:00Z\",\n",
    "        end_date=f\"{year}-12-31T23:59:59Z\"\n",
    "    )\n",
    "\n",
    "    results = unique_indices(results)\n",
    "    print(f\"üéØ Total scenes to process: {len(results)}\")\n",
    "\n",
    "    def process_results_in_parallel(result):\n",
    "        status = process_result(result, existing_s2_dates, CDL_parts_path,\n",
    "                                assets_list, scl_exclude_list,\n",
    "                                s2_file_path, bbox, year, lock)\n",
    "        return status\n",
    "\n",
    "    use_mp = True\n",
    "    if use_mp:\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count(),\n",
    "                                  maxtasksperchild=1) as pool:\n",
    "            statuses = pool.map(process_results_in_parallel, results)\n",
    "    else:\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            statuses = list(executor.map(process_results_in_parallel, results))\n",
    "\n",
    "    successful_scenes += sum(1 for s in statuses if s == 0)\n",
    "    failed_scenes += sum(1 for s in statuses if s != 0)\n",
    "\n",
    "    del results\n",
    "\n",
    "total_time = timedelta(seconds=int(time.time() - start_time))\n",
    "\n",
    "print(\"\\n‚úÖ Done.\")\n",
    "print(f\"‚è±Ô∏è Total time: {total_time}\")\n",
    "print(f\"üì∏ Scenes processed: {successful_scenes}\")\n",
    "print(f\"‚ùå Scenes failed: {failed_scenes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve a tile per scene date example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/15/S/YV/2019/6/S2B_15SYV_20190625_1_L2A/SCL.tif\"\n",
    "output_path = \"../data/S2B_15SYV_20190625_1_L2A_SCL.tif\"\n",
    "\n",
    "with requests.get(url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(output_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"‚úÖ SCL.tif downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scl_path = \"../data/S2B_15SYV_20190625_1_L2A_SCL.tif\"\n",
    "\n",
    "with rasterio.open(scl_path) as src:\n",
    "    scl_array = src.read(1)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(scl_array, cmap='tab20', vmin=0, vmax=11)\n",
    "    plt.colorbar(label='SCL class')\n",
    "    plt.title(\"Scene Classification Layer (SCL) - 2019-06-25\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(scl_array, return_counts=True)\n",
    "scl_stats = dict(zip(unique, counts))\n",
    "print(\"üìä SCL value counts:\", scl_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
