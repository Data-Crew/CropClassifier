{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook demonstrates how to use the Crop Data Layers (CDL) API \n",
    "to download pixel-level crop type data for various bounding boxes and years.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up imports\n",
    "project_root = os.path.abspath(\"..\") \n",
    "sys.path.append(project_root)  \n",
    "\n",
    "# GPU Config\n",
    "from config.gpu.gpu_utils import check_GPU_config\n",
    "\n",
    "gpu_device = check_GPU_config()\n",
    "print(f\"GPU config: {gpu_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.datasources import (\n",
    "    map_cdl_codes_to_rgb_and_text,\n",
    "    CDL_clip_retrieve,\n",
    "    replace_matrix_elements,\n",
    "    read_bytes_plot_clipped_CDL_image\n",
    "    )\n",
    "\n",
    "# Call the function to obtain the code to RGB mapping and code to text mapping\n",
    "code_to_rgb_mapping_dict, code_to_text_mapping_dict, code_mapping_df = map_cdl_codes_to_rgb_and_text()\n",
    "print(code_to_rgb_mapping_dict)\n",
    "print(code_to_text_mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example usage\n",
    "matrix = np.array([\n",
    "    [1, 2, 3],\n",
    "    [3, 1, 2],\n",
    "    [2, 3, 1]\n",
    "])\n",
    "\n",
    "color_dict = {\n",
    "    1: (255, 0, 0),  # Red\n",
    "    2: (0, 255, 0),  # Green\n",
    "    3: (0, 0, 255),  # Blue\n",
    "}\n",
    "\n",
    "new_matrix = replace_matrix_elements(matrix, color_dict, code_to_text_mapping_dict)\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ± Mapping Crop Codes to RGB and Text\n",
    "\n",
    "Each value in the matrix is a **crop code**.  \n",
    "The new matrix stores the **RGB** and **text** combination for each code.  \n",
    "\n",
    "#### ðŸ“Œ Example:\n",
    "\n",
    "```python\n",
    "# Matrix with base codes\n",
    "np.array([\n",
    "    [1, 2, 3],\n",
    "    [3, 1, 2],\n",
    "    [2, 3, 1]\n",
    "])\n",
    "\n",
    "# RGB Representation of Each Crop Code\n",
    "array([\n",
    "    [[255, 0, 0], [0, 255, 0], [0, 0, 255]],  # Red, Green, Blue\n",
    "    [[0, 0, 255], [255, 0, 0], [0, 255, 0]],  # Blue, Red, Green\n",
    "    [[0, 255, 0], [0, 0, 255], [255, 0, 0]]   # Green, Blue, Red\n",
    "])\n",
    "\n",
    "# Text Representation of Each Crop Code\n",
    "array([\n",
    "    ['Corn', 'Cotton', 'Rice'],\n",
    "    ['Rice', 'Corn', 'Cotton'],\n",
    "    ['Cotton', 'Rice', 'Corn']\n",
    "], dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define AOIs and visualize/test retreival via interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of bounding boxes and years that we manually identified around the lower Mississippi delta as candidate \n",
    "# areas for training and validation demo. NASSGEO API requires the bounds in \"EPSG:5070\" in the form of left, bottom, right, top. A conversion \n",
    "\n",
    "bbox_list = [\n",
    "    '426362, 1405686, 520508, 1432630', \n",
    "    '390747, 1195097, 437820, 1284288', \n",
    "    '465073, 1479393, 583994, 1504168', \n",
    "    '549309, 1536066, 592356, 1571061', \n",
    "    '491706, 1510362, 571916, 1531111', \n",
    "    '434104, 1306585, 498520, 1342509', \n",
    "    '414748, 1149262, 439833, 1193858'\n",
    "    ]\n",
    "\n",
    "# A smaller area and year in which data will be sampled at full resolution for a final test (the above areas will be subsampled spatially)\n",
    "bbox_unique_scene = '484932, 1401912, 489035, 1405125'\n",
    "year_unique_scene = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CDL data\n",
    "external_drive_path = \"/media/{external_path_to}/cropdata/US\" # fallback\n",
    "\n",
    "try:\n",
    "    tif_bytes_out = CDL_clip_retrieve(\n",
    "        bbox=bbox_list[0], # if API is down, bbox can be also ignored with None\n",
    "        year=2019,  \n",
    "        local_path=external_drive_path\n",
    "    )\n",
    "    print(\"âœ… Data ready for processing.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Final error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample window\n",
    "read_bytes_plot_clipped_CDL_image(tif_bytes_out, window_size=800, sample_region=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to ignore double cropped regions during training due to complexity of labeling...\n",
    "but will be interesting to see what the algorithm predicts on this dense real-time test for those areas.\n",
    "'''\n",
    "tif_bytes_out_unique_scene = CDL_clip_retrieve(bbox_unique_scene, year_unique_scene)\n",
    "read_bytes_plot_clipped_CDL_image(tif_bytes_out_unique_scene, window_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and visually check Subsampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from io import BytesIO\n",
    "\n",
    "from preprocessing.datasources import (\n",
    "    sample_raster_data,\n",
    "    create_matrix_from_sampled_data,\n",
    "    plot_CDL_clipped_img\n",
    "    )\n",
    "\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Observe sampling CDL operated as expected (downsampled raster values correctly). \"Background\" pixels here are areas where sampling was skipped.\n",
    "'''\n",
    "\n",
    "# Load the original raster and get its shape\n",
    "with rasterio.io.MemoryFile(BytesIO(tif_bytes_out)) as memfile:\n",
    "    with memfile.open() as dataset:\n",
    "        original_shape = dataset.read(1).shape\n",
    "        \n",
    "# Sample the raster, and create a new raster from the sampled data\n",
    "sampled_data, longitudes, latitudes = sample_raster_data(tif_bytes_out, interval=3)\n",
    "downsampled_raster_data = create_matrix_from_sampled_data(sampled_data, original_shape)\n",
    "\n",
    "# Crear una figura con una sola subgrÃ¡fica\n",
    "fig = make_subplots(rows=1, cols=1, subplot_titles=[\"Downsampled CDL Image\"])\n",
    "\n",
    "# Plot the new raster data\n",
    "rgb_image, text_matrix = replace_matrix_elements(downsampled_raster_data, code_to_rgb_mapping_dict, code_to_text_mapping_dict)\n",
    "plot_CDL_clipped_img(downsampled_raster_data[:800, :800], rgb_image[:800, :800], text_matrix[:800, :800], fig, row=1, col=1)\n",
    "\n",
    "# Mostrar la figura despuÃ©s de agregar las trazas\n",
    "fig.update_layout(title=\"Downsampled CDL Data\", width=600, height=600)\n",
    "fig.show()\n",
    "\n",
    "# Calculate pixels corresponding to 'Corn'\n",
    "pixel_count_corn = np.count_nonzero(text_matrix[:800, :800] == 'Corn')\n",
    "print(f\"The 'Corn' corresponds to {pixel_count_corn} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Check that downsampled data covers the same lat/lon extents as original data, but is truly taking a subset of the locations\n",
    "_, orig_lons, orig_lats = sample_raster_data(tif_bytes_out, interval=1)\n",
    "# Plot the raster data and sampled points\n",
    "plt.scatter(orig_lons, orig_lats, color='blue', alpha=1, s=1)\n",
    "plt.scatter(longitudes, latitudes, color='red', s=1, alpha=1)\n",
    "plt.gca().set_xlim(left=orig_lons.min(), right=orig_lons.min()+0.002)\n",
    "plt.gca().set_ylim(bottom=orig_lats.min(), top=orig_lats.min()+0.08)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(orig_lons, orig_lats, color='blue', s=.001)\n",
    "plt.scatter(longitudes, latitudes, color='red', s=.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2019, 2020, 2021]\n",
    "tif_bytes_list = [[CDL_clip_retrieve(x, y) for x in bbox_list] for y in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasources import (\n",
    "    get_crop_stats_from_AOI,\n",
    "    display_table\n",
    ")\n",
    "\n",
    "stats_list = [\n",
    "    [\n",
    "        (*get_crop_stats_from_AOI(t, code_to_rgb_mapping_dict, code_to_text_mapping_dict), s[-1]) \n",
    "        for t in s[0]\n",
    "    ]\n",
    "    for s in zip(tif_bytes_list, years)  \n",
    "]\n",
    "\n",
    "\n",
    "# Concatenating the DataFrames\n",
    "df_stats_list = pd.concat([display_table(x) for x in stats_list], ignore_index=True)\n",
    "display(df_stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stats_list.Year.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize a subsampled scene to make sure it looks reasonable\n",
    "'''\n",
    "# Sample the raster, and create a new raster from the sampled data\n",
    "sampled_data, longitudes, latitudes = sample_raster_data(tif_bytes_list[0][0], interval=10)\n",
    "\n",
    "# Plot the new raster data\n",
    "rgb_image, text_matrix = replace_matrix_elements(sampled_data, code_to_rgb_mapping_dict, code_to_text_mapping_dict)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=1, subplot_titles=[\"Subsampled CDL Image\"])\n",
    "plot_CDL_clipped_img(sampled_data[:, :], rgb_image[:, :], text_matrix[:, :], fig, row=1, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# calculate pixels corresponding to 'Corn'\n",
    "pixel_count_corn = np.count_nonzero(text_matrix[:, :] == 'Corn')\n",
    "print(f\"The 'Corn' corresponds to {pixel_count_corn} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data & Write the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_session import spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datasources import sample_data_into_dataframe_write_parquet\n",
    "\n",
    "# Save a \"dense\" test dataset\n",
    "sample_data_into_dataframe_write_parquet(tif_bytes_out_unique_scene, bbox_unique_scene, year_unique_scene, \n",
    "                                         filename=\"CDL_unique_scene.parquet\", interval=1)\n",
    "\n",
    "# Quick check that the dataframe saved properly by loading & counting the number of rows\n",
    "target_path = os.path.join(project_root, 'data/CDL_unique_scene.parquet/')\n",
    "\n",
    "unique_scene_pt = spark.read.parquet(target_path)\n",
    "unique_scene_pt.show(5)\n",
    "\n",
    "#print(f\"Total number of rows: {dense_parquet.count()}\")\n",
    "print(f\"Total number of rows: {unique_scene_pt.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write all training and validation data out. Downsample by factor of 15 in order to keep overall data size low \n",
    "since Sentinel-2 data for each pixel will increase data size significantly (>100x)\n",
    "'''\n",
    "for l1 in zip(tif_bytes_list, years):\n",
    "    for l2 in zip(l1[0], bbox_list):\n",
    "        sample_data_into_dataframe_write_parquet(l2[0], l2[1], l1[1], interval=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load and display the train/validation data (using some aggregations) to check that it wrote out properly\n",
    "'''\n",
    "target_path = os.path.join(project_root, 'data/CDL_samples.parquet/')\n",
    "sample_df = spark.read.parquet(target_path)\n",
    "result = sample_df.groupBy(\"year\", \"CDL\") \\\n",
    "    .agg(F.count(\"CDL\").alias(\"count\")) \\\n",
    "    .withColumn(\"total_count\", F.sum(\"count\").over(Window.partitionBy(\"year\"))) \\\n",
    "    .withColumn(\"percentage\", F.round((F.col(\"count\") / F.col(\"total_count\")) * 100, 2))\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(10, truncate=False)  # Show first 10 rows without truncating strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.select(\"bbox\", \"year\").distinct().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
