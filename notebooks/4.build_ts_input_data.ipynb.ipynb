{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook demonstrates how to reshape the sampled Sentinel-2 data into a format suitable for modeling. \n",
    "The final dataset contains the same information as the original samples, but is reorganized as one row per pixel and year\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from preprocessing.spark_session import spark\n",
    "from preprocessing.transform_input_data import (\n",
    "    create_image_map,\n",
    "    group_time_series,\n",
    ")\n",
    "\n",
    "# UDFs must be defined in the notebook so that Spark can serialize them\n",
    "from pyspark.sql.types import ArrayType, StringType, BinaryType\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "def get_sorted_input(dicts_list):\n",
    "    sd = sorted(dicts_list, key=lambda x: [*x][0])\n",
    "    only_nums, tiles, img_dates, scl_vals = [], [], [], []\n",
    "    for item in sd:\n",
    "        keyi = [*item][0]\n",
    "        only_nums.extend(item.get(keyi)[:-2])\n",
    "        scl_vals.append(item.get(keyi)[-2])\n",
    "        tiles.append(item.get(keyi)[-1])\n",
    "        img_dates.append(keyi.strftime('%Y-%m-%d'))\n",
    "    return [\n",
    "        ','.join(map(str, only_nums)),\n",
    "        ','.join(map(str, tiles)),\n",
    "        ','.join(img_dates),\n",
    "        ','.join(map(str, scl_vals))\n",
    "    ]\n",
    "\n",
    "def convert_bytes(bands_str: str) -> bytes:\n",
    "    band_vals = b''\n",
    "    for num in bands_str.split(','):\n",
    "        band_vals += int(float(num)).to_bytes(2, 'big')\n",
    "    return band_vals\n",
    "\n",
    "def convert_bytes_scl_vals(scl_vals_in: str) -> bytes:\n",
    "    scl_vals_bstr = b''\n",
    "    for num in scl_vals_in.split(','):\n",
    "        scl_vals_bstr += int(num).to_bytes(1, 'big')\n",
    "    return scl_vals_bstr\n",
    "\n",
    "def convert_string_utf8(s: str) -> bytes:\n",
    "    return s.encode('UTF-8')\n",
    "\n",
    "def date_array_2_int(date_arr: str) -> bytes:\n",
    "    dates = date_arr.split(',')\n",
    "    date_vals = b''\n",
    "    for x in dates:\n",
    "        days = (datetime.strptime(x, '%Y-%m-%d').date() - datetime(1970, 1, 1).date()).days\n",
    "        date_vals += int(days).to_bytes(2, 'big')\n",
    "    return date_vals\n",
    "\n",
    "# Register local UDFs\n",
    "get_sorted_input_udf = F.udf(get_sorted_input, ArrayType(StringType()))\n",
    "convert_bytes_udf = F.udf(convert_bytes, BinaryType())\n",
    "convert_bytes_scl_vals_udf = F.udf(convert_bytes_scl_vals, BinaryType())\n",
    "convert_string_utf8_udf = F.udf(convert_string_utf8, BinaryType())\n",
    "date_array_2_int_udf = F.udf(date_array_2_int, BinaryType())\n",
    "\n",
    "# Main arguments\n",
    "input_uri = \"../data/s2_unique_scene.parquet/\"\n",
    "output_uri = \"../data/CDL_unique_scene_ts.parquet/\"\n",
    "bbox = \"484932, 1401912, 489035, 1405125\"\n",
    "year = \"2019\"\n",
    "\n",
    "# Group pixels by scene date\n",
    "df = spark.read.parquet(f\"{input_uri}bbox={bbox}/year={year}\")\n",
    "df = create_image_map(df)\n",
    "df = group_time_series(df)\n",
    "\n",
    "# Flatten\n",
    "df = df.withColumn(\"inputs_lists\", get_sorted_input_udf(F.col(\"image_dicts_list\"))).drop(\"image_dicts_list\")\n",
    "df = df.withColumn(\"bands\", F.col(\"inputs_lists\")[0])\n",
    "df = df.withColumn(\"tiles\", F.col(\"inputs_lists\")[1])\n",
    "df = df.withColumn(\"img_dates\", F.col(\"inputs_lists\")[2])\n",
    "df = df.withColumn(\"scl_vals\", F.col(\"inputs_lists\")[3])\n",
    "df = df.drop(\"inputs_lists\")\n",
    "\n",
    "# Binary conversion\n",
    "ts = df \\\n",
    "    .withColumn(\"bands\", convert_bytes_udf(F.col(\"bands\"))) \\\n",
    "    .withColumn(\"img_dates\", date_array_2_int_udf(F.col(\"img_dates\"))) \\\n",
    "    .withColumn(\"tiles\", convert_string_utf8_udf(F.col(\"tiles\"))) \\\n",
    "    .withColumn(\"CDL\", convert_string_utf8_udf(F.col(\"CDL\"))) \\\n",
    "    .withColumn(\"scl_vals\", convert_bytes_scl_vals_udf(F.col(\"scl_vals\"))) \\\n",
    "    .withColumn(\"bbox\", F.lit(bbox.encode(\"UTF-8\"))) \\\n",
    "    .withColumn(\"year\", F.lit(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final output\n",
    "ts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it out\n",
    "ts.write.partitionBy(['bbox', 'year']).mode(\"append\").parquet(output_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
