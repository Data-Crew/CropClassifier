{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook demonstrates how to write out training and validation sets.\\\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up imports\n",
    "project_root = os.path.abspath(\"..\") \n",
    "sys.path.append(project_root)  \n",
    "\n",
    "# Enable efficient use of GPU memory\n",
    "from config.gpu.gpu_utils import configure_tensorflow_gpu\n",
    "configure_tensorflow_gpu()\n",
    "\n",
    "from preprocessing.spark_session import spark  # Reuse the preconfigured SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"../data/CDL_multiple_scene_ts.parquet\")\n",
    "df = df.withColumn('CDL', F.decode(F.col('CDL'), 'UTF-8'))\n",
    "\n",
    "df.groupBy('CDL', 'year').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "train_files = glob.glob('../data/CDL_multiple_scene_ts.parquet/*/*2021*/*.parquet')   # 2021 â†’ train\n",
    "val_files   = glob.glob('../data/CDL_multiple_scene_ts.parquet/*/*2020*/*.parquet')   # 2020 â†’ val\n",
    "test_files  = glob.glob('../data/CDL_unique_scene_ts.parquet/*/*2019*/*.parquet')  # 2019 â†’ test\n",
    "\n",
    "print(\"Train files:\", len(train_files))\n",
    "print(\"Val files:  \", len(val_files))\n",
    "print(\"Test files: \", len(test_files))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Hyperparameters and constants\n",
    "\n",
    "# Crops we will identify\n",
    "targeted_cultivated_crops_list = ['Soybeans', 'Rice', 'Corn', 'Cotton']\n",
    "\n",
    "# Crops we identify as \"Cultivated\"\n",
    "other_cultivated_crops_list = [\n",
    "    'Other Hay/Non Alfalfa', 'Pop or Orn Corn', 'Peanuts', 'Sorghum', 'Oats', 'Peaches',\n",
    "    'Clover/Wildflowers', 'Pecans', 'Sod/Grass Seed', 'Other Crops', 'Dry Beans', 'Winter Wheat',\n",
    "    'Alfalfa', 'Potatoes', 'Peas', 'Herbs', 'Rye', 'Cantaloupes', 'Sunflower',\n",
    "    'Watermelons', 'Sweet Corn', 'Sweet Potatoes'\n",
    "]\n",
    "\n",
    "# The label legend\n",
    "label_legend = ['Uncultivated', 'Cultivated', 'No Crop Growing', 'Soybeans', 'Rice', 'Corn', 'Cotton']\n",
    "\n",
    "# Define model batch size and time-series bucketing size \n",
    "BATCH_SIZE = 1028\n",
    "DAYS_IN_SERIES = 120\n",
    "DAYS_PER_BUCKET = 5\n",
    "MAX_IMAGES_PER_SERIES = (DAYS_IN_SERIES // DAYS_PER_BUCKET) + 1\n",
    "FRAMES_TO_CHECK = 2\n",
    "BUCKETING_STRATEGY = \"random\"\n",
    "NUM_FEATURES = 16 # 12 bands + 4 indices (or 12 for bands only)\n",
    "\n",
    "print(\"ðŸ”¢ MAX_IMAGES_PER_SERIES:\", MAX_IMAGES_PER_SERIES)\n",
    "print(\"ðŸ“¦ Batch shape: [{} x {}]\".format(BATCH_SIZE, MAX_IMAGES_PER_SERIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series bucketing \n",
    "\n",
    "`MAX_IMAGES_PER_SERIES` is calculated based on two parameters: `DAYS_IN_SERIES` and `DAYS_PER_BUCKET`.  \n",
    "It represents the maximum number of time steps (i.e., satellite images or observations) per pixel over a year.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "`MAX_IMAGES_PER_SERIES = (DAYS_IN_SERIES // DAYS_PER_BUCKET) + 1`\n",
    "\n",
    "*Examples:*\n",
    "- If `DAYS_IN_SERIES = 120` and `DAYS_PER_BUCKET = 5`, then `MAX_IMAGES_PER_SERIES = 25`\n",
    "- If `DAYS_IN_SERIES = 100`, then `MAX_IMAGES_PER_SERIES = 21`\n",
    "\n",
    "The `BATCH_SIZE` parameter defines how many pixels are processed **in parallel** during each training step.  \n",
    "It refers to different spatial points (locations) in the dataset.  \n",
    "Each \"pixel\" here means one location with its own full time series of features.\n",
    "\n",
    "---\n",
    "\n",
    "#### What does a single pixelâ€™s time series look like?\n",
    "\n",
    "Each pixelâ€™s time series is a sequence of feature vectorsâ€”e.g., values for NDVI, red band, NIR band, etc.â€”captured at different time steps:\n",
    "\n",
    "```python\n",
    "[\n",
    "  [0.2, 123],\n",
    "  [0.3, 118],\n",
    "  [0.35, 110],\n",
    "  [0.4, 100],\n",
    "  [0.45, 95]\n",
    "]  # shape: (5, 2) = (time_steps, features)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "* There are 5 time steps (observations)\n",
    "\n",
    "* Each step includes 2 features (e.g., NDVI and surface temperature)\n",
    "\n",
    "With a size of 1028 pixels and 5 time steps:\n",
    "\n",
    "```python\n",
    "batch = np.array([\n",
    "    [[...], [...], [...], [...], [...]],  # pixel 1\n",
    "    [[...], [...], [...], [...], [...]],  # pixel 2\n",
    "    ...\n",
    "    [[...], [...], [...], [...], [...]]   # pixel 1028\n",
    "])  # shape: (1028, 5, 2) = (batch_size, time_steps, features)\n",
    "```\n",
    "\n",
    "#### Parameter Summary\n",
    "\n",
    "| Parameter              | Meaning                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------|\n",
    "| `BATCH_SIZE`           | Number of distinct pixels (locations) processed per batch               |\n",
    "| `MAX_IMAGES_PER_SERIES`| Maximum number of time steps per pixel (e.g., 25 images per year)       |\n",
    "| `features`             | Number of bands or indices per time step (e.g., NDVI, red, nirâ€¦)        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate mean and stdv to normalize band values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from dataloader import make_from_pandas, filter_double_croppings, parse\n",
    "\n",
    "train_files_ds = make_from_pandas(train_files)\n",
    "\n",
    "# Set the normalization flag to False to get the un-normalized data\n",
    "non_normed_ds = (\n",
    "    train_files_ds\n",
    "    .filter(filter_double_croppings)\n",
    "    .map(lambda x: parse(\n",
    "        x,\n",
    "        norm=False,\n",
    "        means=tf.zeros([NUM_FEATURES], dtype=tf.float32), \n",
    "        stds=tf.ones([NUM_FEATURES], dtype=tf.float32),\n",
    "        label_legend_=label_legend,\n",
    "        targeted_cultivated_crops_list=targeted_cultivated_crops_list,\n",
    "        other_cultivated_crops_list=other_cultivated_crops_list,\n",
    "        days_in_series=DAYS_IN_SERIES,\n",
    "        days_per_bucket=DAYS_PER_BUCKET,\n",
    "        max_images_per_series=MAX_IMAGES_PER_SERIES,\n",
    "        frames_to_check=FRAMES_TO_CHECK,\n",
    "        bucketing_strategy=BUCKETING_STRATEGY\n",
    "    ), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# Loop through the dataset, saving both the data and associated labels\n",
    "all_non_normalized_data = []\n",
    "all_labels = []\n",
    "\n",
    "for data, label in non_normed_ds:\n",
    "    all_non_normalized_data.append(data)\n",
    "    all_labels.append(label)\n",
    "\n",
    "# Reshape to just get the imagery values - no need to maintain the time-series structure for the following plots\n",
    "num_features = 18 # 12 bands + 4 indices + 1 SCL + 1 label\n",
    "all_non_normalized_data = tf.reshape(tf.concat(all_non_normalized_data, axis=0), shape=(-1, num_features))\n",
    "\n",
    "all_labels = tf.reshape(tf.concat(all_labels, axis=0), shape=(-1, len(label_legend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = tf.math.reduce_mean(tf.ragged.boolean_mask(all_non_normalized_data, mask=(all_non_normalized_data!=0)), axis=0)\n",
    "stds = tf.math.reduce_std(tf.ragged.boolean_mask(all_non_normalized_data, mask=(all_non_normalized_data!=0)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means[0:NUM_FEATURES] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds[0:NUM_FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import make_dataset\n",
    "\n",
    "train_ds, val_ds = make_dataset(\n",
    "    train_files,\n",
    "    val_files,\n",
    "    method=\"pandas\",  # or \"tensorflow\"\n",
    "    batch_size=BATCH_SIZE,\n",
    "    means=means[0:NUM_FEATURES],\n",
    "    stds=stds[0:NUM_FEATURES],\n",
    "    label_legend=label_legend,\n",
    "    targeted_cultivated_crops_list=targeted_cultivated_crops_list,\n",
    "    other_cultivated_crops_list=other_cultivated_crops_list,\n",
    "    days_in_series=DAYS_IN_SERIES,\n",
    "    days_per_bucket=DAYS_PER_BUCKET,\n",
    "    max_images_per_series=MAX_IMAGES_PER_SERIES,\n",
    "    frames_to_check=FRAMES_TO_CHECK,\n",
    "    bucketing_strategy=BUCKETING_STRATEGY,\n",
    "    augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.save(f\"../data/train_ds_with_idx_{NUM_FEATURES}f\")\n",
    "val_ds.save(f\"../data/val_ds_with_idx_{NUM_EATURES}f\")\n",
    "\n",
    "print(\"ðŸ’¾ Datasets saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Output: features & one hot labels\n",
    "X, y = next(iter(train_ds))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "col_names = ['coastal', 'blue', 'green', 'red', 'rededge1', 'rededge2',\n",
    "             'rededge3', 'nir', 'nir08', 'nir09', 'swir16', 'swir22',\n",
    "             'NDVI', 'EVI', 'NWDI', 'NDBI']\n",
    "\n",
    "# Normalized data\n",
    "#all_normalized_data = tf.reshape(tf.concat([d[0] for d in train_ds], axis=0), shape=(-1, 12))\n",
    "all_normalized_data = tf.reshape(tf.concat([d[0] for d in train_ds], axis=0), shape=(-1, 16))\n",
    "df_norm = pd.DataFrame(all_normalized_data.numpy(), columns=col_names)\n",
    "df_norm = df_norm.drop_duplicates()  # Ignore padded rows\n",
    "\n",
    "# Non-normalized data\n",
    "#df_non_norm = pd.DataFrame(all_non_normalized_data[:, 0:12], columns=col_names)\n",
    "df_non_norm = pd.DataFrame(all_non_normalized_data[:, 0:16], columns=col_names)\n",
    "df_non_norm = df_non_norm[df_non_norm != 0].dropna()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))  \n",
    "\n",
    "sns.violinplot(data=df_norm, ax=ax1)\n",
    "ax1.set_xticklabels(col_names, rotation=90)\n",
    "ax1.set_title('Normalized Dataset')\n",
    "ax1.set_xlabel('Feature')\n",
    "ax1.set_ylabel('Data Value')\n",
    "ax1.set_ylim((-2.5, 2.5))  # Optional\n",
    "\n",
    "sns.violinplot(data=df_non_norm, ax=ax2)\n",
    "ax2.set_xticklabels(col_names, rotation=90)\n",
    "ax2.set_title('Non-Normalized Dataset')\n",
    "ax2.set_xlabel('Feature')\n",
    "ax2.set_ylabel('Data Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "heights = tf.argmax(all_labels, axis=1).numpy()\n",
    "plt.bar(label_legend, np.histogram(heights, bins=len(label_legend))[0])\n",
    "plt.title('Crop Types in Training Set')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl_mapper = {\n",
    "    0.0: 'No Data',\n",
    "    1.0: 'Saturated Or Defective',\n",
    "    2.0: 'Dark Area Pixels',\n",
    "    3.0: 'Cloud Shadows',\n",
    "    4.0: 'Vegetation',\n",
    "    5.0: 'Not Vegetated',\n",
    "    6.0: 'Water',\n",
    "    7.0: 'Unclassified',\n",
    "    8.0: 'Cloud Medium Probability',\n",
    "    9.0: 'Cloud High Probability',\n",
    "    10.0: 'Thin Cirrus',\n",
    "}\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15,15))\n",
    "i = 0\n",
    "for data, label in non_normed_ds:\n",
    "    df = pd.DataFrame(data.numpy()[i,:,[-3, -2]].T, columns=['NDVI', 'SCL'])\n",
    "    df['image in series'] = np.arange(0, df.shape[0], step=1)\n",
    "\n",
    "    df['SCL Label'] = df.SCL.map(scl_mapper)\n",
    "    sns.scatterplot(data=df, x='image in series', y='NDVI', hue='SCL Label', ax=axs[i//3, i%3])\n",
    "    axs[i//3, i%3].set_title(label_legend[tf.argmax(label[i,:]).numpy()])\n",
    "    \n",
    "    i += 1\n",
    "    if i == 9:\n",
    "        break\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
